1 Introduction

• How is a complete transformation characterized? How can we further simplify it?
- Given a rotation matrix R = [cos(\theta), -sin(\theta); sin(\theta), cos(\theta)], a point p = [a; b], and a translation vector t = [t_c; t_r], a complete transformation is Rp + t = [a*cos(\theta) - b*sin(\theta) + t_c; a*sin(\theta) + b*cos(\theta) + t_r]. \nTransformation can be TP where T = [cos(\theta), -sin(\theta), t_c; sin(\theta), cos(\theta), t_r; 0, 0, 1] and P = [a; b; 1].

• Given a 2 joint robot arm with T_w(\alpha) the wrist joint position relative to the lower arm, T_l(\beta) the lower arm position relative to the upper arm, and T_0 the initial position, where in the real world (scene) is a wrist coordinate point x at the tip of the robot? Can we find x if we know the real world coordinate y?
- y = T_0 * T_l(\beta) * T_w(\alpha) * x. \nYes, by inverting the operation: x = (T_w(\alpha))^{-1} * (T_l(\beta))^{-1} * (T_0)^{-1} * y

• How do we represent line and arc segments in polycurve modelling?
- (a, b) - L - (c, d) for line, (a, b) - arc(x, y) - (c, d) for arcs. 

• What are some issues concerning visual ethics?
- Video surveillance, autonomous navigation, factory automation, biometrics and car registration plate reading.

2 Flat Part Recognition 

• How do we do geometric model-based rigid object recognition? What assumptions are made?
- 4 stages: \n1. Geometric description. \n2. Model matching. \n3. Pose estimation. \n4. Match verification. \nWe assume we are given an isolated binary image object (segmented), that we have access to a geometric model of the object and that we can extract the edges from the image.

• Why is the method seen suitable for an estimated 50% of manufactured flat parts?
- Because most parts will have a few simple features like straight edges or circular holes. These are required for attaching the part to a larger assembly. With a small number (e.g. 2-3) of these features, we could recognise and estimate the pose of the parts.

• How do we find the boundary of an object?
- 1. Get the points that lie on the boundary. \n2. Remove any spurs on the boundary (clean up). \n3. Track (order pixels). \n4. Segment (straight line segments).

• What is a spur? Is this the only kind of pixel we'd want to remove?
- Any boundary pixel with only 1 neighbor inside a 3x3 neighborhood. \nNo, also surplus pixels such as in the exterior of a diagonal connection.

• How does consecutive boundary tracking work?
- Center the current pixel in the middle of a 3x3 matrix numbered 1-8 clockwise. Compute the first pixel to look for as NEXT = (LAST + 3 + i)MOD 8 + 1, then keep looking clockwise.

• How does recursive splitting the boundary into linear segments work?
- 1. Find leftmost point A. \n2. Find rightmost point B. \n3. Split points in set A -> B and B -> A: \n3.1 Find line through current segment endpoints X & Y. \n3.2 Find point Z furthest from the line at distance d. \n3.3 If d is less than a threshold, then this segment is finished. \n3.4 Otherwise, create new sets X -> Z and Z -> Y and recurse.

• Which point are we looking for when executing the curve segmentation algorithm? What do we do at this point?
- The point that is the furthest from the straight line connecting the endpoints of the set of points. If the distance to the line is large enough, we call this a corner point and split the set at this point.

• How does the Interpretation Tree Model Matching work?
- Given a set of model lines {m_i} (scene coords.), a set of image lines {d_j} (image coords.) and an image to scene scale conversion factor \sigma, match the lines {(m_i, d_j)}, estimate transformation mapping model onto data (R, t), and verify matching and pose estimate, outputting identity and position (R,t). 

• How does the matching phase work in the ITMM?
- Depth-first search on possible correspondences between {m_i} and {d_j}, extended with a wildcard '*' (to match features without corresponding data features or occlusion, segmentation failures).

• How do we reduce search complexity for matching (how to prune)?
- Assuming we've reached level k (k matched pairs), there's 4 ways we can add new pair (m_{k+1}, d_{j_{k+1}}): \n1. Unary test: check whether pair is compatible. \n2. Binary test: check whether pair is compatible with each other and the pairs in the tree.  \n3. Early success limit L: stop search when we have L compatible pairs. \n4. Early failure limit L: stop search when it is not possible to get L pairs on this path. If t non-wildcard matches out of k, fail if t + (M - k) < L.

• What are good unary/binary properties to test if matching parts with sets of circular holes?
- Unary: lengths of lines, angles of corners, radii of holes. \nBinary: angles between lines, distance between points like corners and holes, distance between parallel lines.

• How does 2D Pose Estimation work?
- Given a set {(m_i, d_{j_i})} of compatible pairs, want to find the rotation R and translation t that transforms the model onto the data features. \nTo estimate the rotation: compute unit vectors for model and data lines (u_i = \frac{m_{i2} - m_{i1}}{||m_{i2} - m_{i1}||}, and same for v_i). If no errors could do v_i = +- Ru_i, but since we know there will be errors we find the least squares solution. Doing so is done by 1) computing vectors perpendicular to v_i (-v_{yi},v_{xi}), 2) compute error between v_i and Ru_i using dot product of Ru_i and perpendicular, 3) reformulate least squares error, and 4) find rotation that minimizes least square error, and verify all 4 solutions. \nSame for translation as \sigma * R * m + t.

• How does 2D Shape Matching verification work?
- Geometric verification by checking alignment (direction, distance, overlap) of model and data lines through testing 1) are they parallel? - dot product of unit vectors should be close to 1 or -1, 2) are they close? - error less than a threshold, where the error is e_i = (d_k - m_{i1})'w_i, w_i is the perpendicular to the model unit vector, 3) do they overlap? - want a \lambda close to 0, where \lambda = (d_k - m_{i1})'u_i, then -tolerance||m_{i1} - m_{i2}|| <= \lambda_k <= (1 + tolerance)|| m_{i1} - m_{i2}||. \nFinally, evaluate with a confusion matrix.

3 Detection and Tracking objects in video

• Why would we want to do detection and tracking in videos?
- Human sign language recognition, vehicle monitoring, overcrowding, sports, exclusion zones, animal behaviour and health monitoring.

• What assumptions do we make in detection and tracking of the falling ball? What issues do we need to deal with?
- A constant background, large contrast with object and background, and newtonian motion model. \nMotion blur and the bounce.

• Why do we need a robust method of estimating mean and stds? How do we obtain a robust estimation of the mean and std of a set of values {x_t}?
- To deal with outliers. \nAssuming they come from the same normal distribution, use the median for the mean (\mu = median({x_t})), and for the std use \sigma = \frac{m}{0.68\sqrt 2} where m = median({x_t - x_{t+1}})

• What is a simple target detector if you have no prior information about the video, and what are its issues?
- Image Differencing, issues due to illumination changes, overlapping changes and scene vibrations.

• What constraints must be satisfied for background subtraction? What expression characterizes this process?
- 1. Camera is stationary with no autofocus/gain. \n2. Illumination is constant. \n3. Largely isolated moving objects. \n |current - background| > threshold 

• What are some change detection issues encountered with a fixed background?
- Gradual or rapid illumination changes (sun movement, lights on/off), background object shadow, camera jitter, halting objects like parked cars.

• What are chromaticity coordinates? Using these coordinates how do we find the lightness changes?
- Normalized RGB. Since r + g + b = 1 just use (r,g). \nLightness can be computed as s = (R + G + B) / 3, and included as (r_t, g_t, s_t) for model pixel at time t, (r_B, g_B, s_B) as model background. If s_t/s_B < \alpha or s_t/s_B > \beta then foreground otherwise background. 

• How is a probabilistic model of the background characterized? How do we add chromaticity to the model?
- As a non-parametric distribution: Pr(x | Background) = \frac{1}{N}\sum_{i=1}^N K_{\sigma}(x - b_i) where b_i are previous samples from the background and gaussian kernel function K_{\sigma}(x) = \frac{1}{\sigma\sqrt{2\pi}} e^{-\frac{x^2}{2\sigma^2}}. \nSet x = (r,g) and assuming independence compute the product over {r,g} as Pr(x | Background) = \frac{1}{N}\sum_{i=1}^N \prod_{j \in {r,g}} K_{\sigma}(x_j - b_{ij}).

• How do we keep updating our probabilistic model (effectively dealing with slow drift in illumination/multiple backgrounds due to jitter)?
- At each pixel i, keep N most recent background pixel values (throw away oldest and replace).

• Where is the most variation in background pixel values?
- 1. Where there is variation in illumination \n2. Where there are moving leaves \n3. Where shadow edges move \n4. At the edges of video compression blocks.

• How do we remove noise from a thresholded (binary) foreground image? Why do we need to do some noise cleaning as part of detection?
- 1. Collect into regions by 4-connectedness \n2. Remove groups with less than 5 pixels \n3. 'Close' (dilate then erode) to fill in gaps \n4. Remove groups with less than 20 pixels. \n Because: 1) a low threshold (to be sensitive to change) means that we also detect some noise as if it were a change, 2) camera vibration and compression artifacts are detected near intensity edges.

• What is the sequence of (high-level) actions in a Model-Based Tracker? What are the key techniques used?
- Verify location of object in scene, update our model with the state of the object, predict location in next frame, repeat. \nKalman filter or condensation tracking with a motion model that can be learned or calibrated as a parametric model such as Newton's Laws of Motion (x(t) = s_0 + tv_0 + \frac{1}{2}t^2a).

• What are the limitations of Model-Based tracking?
- Objects come and go, lighting changes, shadows, leaves, occlusions, fast objects.

• What is the Kalman filter useful for? What can we do with a model based on Kalman filters? What are the two main models needed by the standard Kalman Filter?
- Integrating evidence over time, great for tracking problems. It is a statistical model, keeps track of mean and covariance. \nPredict likely position, reducing search; no need to have accurate estimates since they are well integrated together. Essentially smooths noise observations to give better estimates but fails in some cases like a ball bouncing and stopping. \n1. The process model, which specifies how the state updates over time. \n2. The observation model, which specifies the relationship between the current state and what is observed.

• What are the components of the Kalman filter? How does the Kalman filter algorithm work?
- The components: \n1. State vector x_t with associated state transition matrix A_t. \n2. Process model which updates the state: x_t = Ax_{t-1} + Bu_{t-1} + w_{t-1} where Bu is some external control and noise w is a multivariate normal with mean 0 and covariance Q. \n3. Observation model (relates measured data z to current state): z_t = Hx_t + v_t where H extracts observations and noise v is a multivariate normal with mean 0 and covariance R. \nThe algorithm: \n1. Predict likely state: y_t = Ax_{t-1} + Bu_{t-1}. \n2. Estimate error of predicted state: E_t = AP_{t-1}A' + Q. \n3. Estimate correction gain between actual and predicted observations: K_t = E_tH'(HE_tH' + R)^{-1}. \n4. Estimate new state: x_t = y_t + K_t(z - Hy_t). \n5. Estimate the error of new state: P_t = (I - K_tH)E_t

• How do we model a bouncing ball using a Kalman filter for tracking?
- Position: p_t = (col_t, row_t)' \nVelocity: v_t = (velcol_t, velrow_t); \nPosition update: p_t = p_{t-1} + v_{t-1}\Delta t \nVelocity update: v_t = v_{t-1} + a_{t-1}\Delta t \nAcceleration: a_t = (0, g)' \nState vector: x_t = (col_t, row_t, velcol_t, velrow_t)', initialize randomly \nPrediction: y_t = Ax_{t-1} + Bu_t where A = [1 0 \Delta t 0; 0 1 0 \Delta t; 0 0 1 0; 0 0 0 1] and Bu_t = [0; 0; 0; g\Delta t] with \Delta t = 1 \nObservation process: H = [1 0 0 0; 0 1 0 0] \nNoise measurement: R = [0.285 0.005; 0.005 0.046] \nSystem noise: Q = 0.01 x I

• How might you correct the tracking failure that happens when the ball bounces using a Kalman filter?
- Switch to a different behaviour model, the falling ball model no longer applies at the time of the bounce.

• What does condensation tracking stand for? What is the core idea? 
- Conditional Density Propagation aka Particle Filtering. \nIt maintains a set of multiple hypotheses with associated estimated probabilities, probabilistically generate new hypotheses from the set, update those using new data from changes in the state and behaviour and update hypothesis probabilities. 

• Lay out the condensation tracking algorithm
- Given a set of N hypotheses at time t-1, H_{t-1} = {x_{1,t-1}, x_{2,t-1}, ..., x_{N,t-1}} with associated probabilities {p(x_{1,t-1}), p(x_{2,t-1}), ..., p(x_{N,t-1})}, repeat N times to generate H_t: \n1. Randomly select x_{k,t-1} from H_{t-1} with probability p(x_{k,t-1}). \n2. Generate new state vector s_{t-1} from a distribution centered at x_{k,t-1}. \n3. Get new state vector using dynamic model x_t = f(s_{t-1}) and Kalman filter. \n4. Evaluate probability p(z_t|x_t) of observed data z_t given state x_t. \n5. Use Bayes rule to get p(x_t|z_t)

• What are the advantages of having multiple hypotheses? Why does condensation tracking work?
- 1. Each hypothesis can represent a slightly different variation, which might fit the data better. \n2. Each hypothesis can represent a different state or model, allowing different behaviours to be tracked until it is clear which is the correct model. \n1) since there are many different hypotheses maybe get one that fits better, 2)a dynamic model can introduce various effects and 3) sampling by probability weeds out bad hypotheses and generating by probability introduces corrections.

• How do we model a bouncing ball using condensation tracking?
- 1. Select a ball motion vector by its probability (N = 100 samples). \n2. Use estimated covariance P() to create state samples s_{t-1}. \n3. Situation switching model (Bounce - Freefall - Stop), P_b = 0.3, P_s = 0.05, P_f = 1 - P_b - P_s. STOP -> zero vertical speed, BOUNCE -> v_{row} *= -0.7 and use Kalman filter.\n4. Estimate hypothesis goodness 1/||Hx_t - z_t||^2 (normalize to estimate probability).

• What are some limitations and extensions of tracking?
- 1. Adaptive background modelling requires a lot of memory. \n2. Need to deal with adapting to rapid illumination changes. \n3. Image compression introduces noise. \n4. Could suppress moving groups. \n5. Could develop foreground statistical models.

• What are some of the issues that arise when using an adaptive background model (at least as described in these lectures)?
- Keeping multiple examples of the background at each pixel can require a lot of memory, and also a lot of computation to compute the probability that an observed pixel is explained by the background model.

4 Range data based 3D part recognition 

• What defines a wedge? How do we model one in 3D? How do we specify its rotation and translation?
- Planar surfaces, most at 90 degrees, one oblique surface. \nModel the shape with 3D coordinates of the points in the objects (defining the set of planes) as a set of edges. \nTranslation as a 3D vector t = (t_x, t_y, t_z)', Rotation needs 3 values and many possible ways to encode them R = R_x(\theta_x)R_y(\theta_y)R_z(\theta_z), each R is like before but with a 1 in the specific axes. 

• How do we represent range data?
- Either as a range image (encoding depth), (r,c) is the pixel location; or a point cloud {(x,y,z)}

• Why would we want to do active 3D sensing?
- Analysis and manufacture, reverse engineering of parts/objects; 3D VR, change analysis in buildings; on-board laser scanner in robot navigation

• What are the advantages and disadvantages of range data?
- Advantages: direct and accurate 3D scene info, unambiguous measurement (as opposed to brightness), kinect sensor is cheap and reliable. \nDisadvantages: more complex and expensive sensor, dark or shiny objects could be a problem and geneally indirect capture (eg. computed, scanned)

• How do triangulation range sensors work? Why is it convenient to use lasers in this case?
- Right laser emits light onto object with angle \beta, left sensor observes the reflected light with angle \alpha, the resulting point from the scene is computed as z = f(\alpha, \beta, D) where D is the length between both sensors. \nIt's bright, tuned to single frequency (eg 633nm), other scene light can be eliminated through matching the optical filter.

• How do we obtain a point x in the scene through triangulation range? How do we extend this method to a whole line? What about the whole scene?
- Ray equation: x = c + \labda(p - c) where p is the pixel on the laser stripe, c the camera origin \nLight plane equation: x * n = d \nFind intersection, solve for \lambda \nSubstitute \lambda in to get x in the ray equation \nUse a half-cylindrical lens that projects a stripe on the target. \n3 possibilities: sweep light plane with rotating mirror, move the sensor or move pars underneath stripe (eg. conveyor belt).

• How do LIDR/Flash-LIDR sensor retrieve a range image? What about phase-delay type sensors?
- Large flash of light, hits the target and based on how fast the light comes back to the sensors we can compute the distance (time of flight) to the pixels. \nThe laser is split in half by a half-silvered mirror that comes back to the sensor, then the reflected light is matched up and the delay between them is computed.

• How can we solve the issue of incomplete range data from capturing laser stripes?
- Could capture from different directions and merge, or infer missing data from observed data.

• What does cosine shading do?
- It colours the pixels according to how much the surface normals of the object points towards the viewer or light source.

• What does the Kinect sensor do?
- It produces a 2D random dot pattern projected onto the scene and from the patterns you can calculate objects in the scene using triangular range methods. Typical image is 640x480, captured from 1-4m, typical depth point spacing 1x1x2mm. Can get RGB for each pixel but unsynced with depth image (as opposed to RGB-D) and is cheap and reliable. 

• What are the differences between range image and point clouds when segmenting planes?
- In range images there are obvious neighbour relations and easier region growing algorithms. In 3D point clouds neighbour relations are in 3D, good data structures (kd-trees) can help with neighbour connections.

• How do we select the new points when we extend the boundary of the planar patch? State the concrete principles we follow
- The point x: \n1) has to be one of the measured 3D points (i.e. it’s not an arbitrary point), \n2) it has to lie close to the infinite plane that best fits the current point set, and \n3) it has to be close to one of the points already in the current point set (i.e. it’s connected to the previously found points) \nGiven plane S: nx + d = 0, add a new point y to S if: \na) |ny + d| < \tau_P \nb) there is a point z in S st. ||y - z|| < \tau_n

• How do we do plane fitting?
- Given a set of datapoints {x_i}, find the n and d that best fit nx_i + d = 0 for all i. We do this by: \nExtend data: y_i = [x_i,1] \nExtend parameters: p = [n,d] \nPlane equation is now: y_ip = 0 \nLeast squared error: \sum_i(y_ip)^2 = \sum_ipy_iy_ip = p(\sum_iy_iy_i)p = pMp \nThen eigenvector of smallest eigenvalue of M is the desired parameter vector, provided eigenvalue is sufficiently small.

• What does the 3D recognition pipeline look like?
- (Range Data =>) Segmentation -> (Geometric Model =>) Model Matching -> Pose Estimation -> Verification (=> Recognition: T/F; Position: t/R) -> (Geometric Model =>) Model Matching etc.

• What are the main differences between the Interpretation Tree for matching in 2D and in 3D?
- Unary constraint is surface area instead of length, binary constraint angle between vectors (surface normals) instead of between the lines. In 3D can add a trinary constraint checking the sign of vector triple product a x (b x c) on surface normals.

• How do we do Pose Estimation in 3D?
- First assume N paired planes {(M_i, D_i)}_{i=1}^N, model {m_i} and data normals {d_i}, a point on each model patch {a_i} and a point on each data patch {b_i} (need not correspond to a_i). \nRotation Estimation: want R such that Rm_i = d_i by minimizing \sum_i||Rm_i - d_i||^2. Form matrix M = [m_1m_2...m_n], D = [d_1d_2...d_N]. Compute svd(DM') = U*S*V' and R = V*U' (assumes at least 3 non-coplaner vectors amongst the dataset). \nTranslation Estimation: minimize the perpendicula separation \lambda_i between rotated model patch and data patch. Goal is to find t that minimizes \sum_i\lambda_i^2. Form matrix L = \sum_id_id_i', vector n = \sum_id_id_i'(Ra_i - b_i) and compute t = -(L)^{-1}n

• How does Verification work in 3D?
- 2 tests: \n1. Are the rotated model normals m_i close to data normals d_i? - acos(d_i'Rm_i) < \tau_1 \n2. Do all the planes lie close to each other? nx + d = 0: |ne_i + d| < \tau_2  where e_i are the transformed model vertices

• In what way is 3D pose estimation like 2D pose estimation?
- We did a least square rotation estimation and then a least square translation estimation, both to minimise the effect of slight errors in data measurement.

• What is a good alternative to planes for primitives from range data?
- Edges like the blade edge (depth changes), fold edge (surface orientation changes) or from changes in the surface's curvature properties. Blade and fold are usable for recognition but are less reliable to extract than surfaces.

5 Stereo data based 3D part recognition 

• What is the core of binocular stereo vision? What is it useful for? What is the key principle?
- Given two 2d images of an object, how can we reconstruct 3D awareness of it? \nObstacle avoidance, grasping, object location. \nKey principle is triangulation.

• What kinds of features are used for binocular stereo vision? Can any of them match in regions without texture?
- Edge fragments (from canny edge detector), edge structures, general interest points (SIFT) and image intensity patches. \nNo.

• What are SIFT features? What are the 4 steps of the algorithm?
- 128-D vectors of image points and local description. Sparse, reasonably distinguishable points that are invariant to translation, rotation, scale and some 3D. \n1. Detect extremal points: convolve image with Gaussian and compute a difference of Gaussians at different scales. Select extremal points larger/smaller than their 26 neighbours (9x3). \n2. Localize (optimal) extremal points. Want a more precise position. Compute the Hessian of the DoG found previously and find a better estimate by subtracting the product of the inverse Hessian and Jacobian of (x,y,\sigma). Remove noise by a) throwing away those with low contrast, those where their predicted DoG (DoG added to the product of gradient and offset) value smaller than a threshold (p < 0.03), b) throwing away those unstable, if negative 2D hessian. \n3. Feature orientation estimation. Rotation invariance. Construct a histogram of the angles \theta for all the features weighted by m, the strength of the gradient at each point, and pick top direction \theta. \n4. Compute descriptors. Calculate HOG, 4 gradient histograms from 8x8 neighbourhood around each feature point (16x16 total, 8 orientations/directions), stacked as a 128-D. Get rotation invariance by rotating all orientations by \hat \theta, and illumination invariance by normalizing vector to unit length. One can then compute similarity by Euclidean distance.

• What are the properties of SIFT features? Why does SIFT use points that are extrema with respect to 26 neighbours?
- Sparse and distinctive, translation independent (by local histogram), rotation independent (by orientation adjustment), scale independent (by descriptor normalisation), widely used and real-time implementaion possible. \nBecause they are invariant to translation, rotation, illumination and scale transformations: \nTranslation: if the image translates, then all the DoG values also translate equally, maintaining the extrema. \nRotation: if the image rotates, then all the DoG values also rotate equally, maintaining the extrema. \nIllumination: if the image gets brighter, the DoG values all increase relatively, so the extrema remains. \nScale: if the image gets larger, then the same relative values of DoG will be calculated, but for a larger value of σ, so the extrema remains

• How do we model image projections in stereo? How does the projection matrix decompose?
- Using the pinhole camera model: A projection matrix P_I projects a 3D point v = (x,y,z,1)' onto image point u_i = (c_i,r_i,1)' for i = L,R as \lambda_i u_i = P_i \nP_i = K_iR_i[I] - q_i where R_i is the camera's rotation matrix in world coords., q_i = (q_{xi}, q_{yi}, q_{zi})' is the camera centre poisition in world coords., and K_i = [f_im_{ri} s_i r_{0i}; 0 f_im_{ci} c_{0i}; 0 0 1] is the camera's intrinsic calibration matrix where f_i is the camera focal length in mm, m_{ri}, m_{ci} are the row, col pixels/mm conversion on image plance, r_{0i}, c_{0i} are where the optical axis hits image plane, and s_i is the skew factor.

• What does epipolar geometry tell us?
- Provides a model for more than 1 camera, says that points p_l and p_r are linked by a common 3D point p in the scene by a fundamental matrix F st. p_rFp_l = 0.

• How do we get the line intersection?
- If image line is a x col + b x row = d, then v_1 = (a,b,d). Intersection with v_2 is v_1 x v_2 (cross product = [v_1]_xv_2 = [0 -d b; d 0 -a; -b a 0]v_2)

• How do we estimate the fundamental matrix (direct linear algorithm)?
- Assume N \geq 8 matched points u_i:v_i, i = 1...N which should satisfy u_iFv_i = 0. Unroll u_i and v_i into A_i and multiply with unrolled F as A_if = 0. Then stack up all A_i so that Af = 0 and solve for f: svd(A) = UDV', f = V(:,9) and adjust so that det(F) = 0. 

• What are epipoles?
- Points in the line connecting the two camera centres intersecting the image planes. They also have a relationship with the fundamental matrix, e_rF = Fe_l = 0.

• How do we estimate projection matrices?
- Given intrinsic parameter matrices K_L, K_R and fundamental matrix F, compute Essential Matrix E = K_R'FK_L, define auxiliary W (= [0 1 0; -1 0 0; 0 0 1]). Decompose E as svd(E) = [U S V]. Compute P_R = [UWV'|U(:,3)]. To account for ambiguity set P_L = [1 0 0 0; 0 1 0 0; 0 0 1 0]. Can generate same projections with an arbitrary projective transformation H as P_LH, P_RH.

• What are the 4 stages of a canny edge detector?
- 1. Gaussian smoothing to reduce noise and smooth away small edges (convolve with 2D Gaussian mask(r,c) = \frac{1}{2\pi\sigma}e^{-\frac{r^2 + c^2}{2\sigma^2}}, larger \sigma gives more smoothing). \n2. Gradient locates potential edge areas, need gradient (nablaG(r,c) = (G_r(r,c), G_c(r,c)), where G_r(r,c) \approx G(r+1,c) - G(r,c)), magnitude of the gradient (H(r,c) = \sqrt{G_r(r,c)^2 + G_c(r,c)^2} \approx |G_r(r,c)| + |G_c(r,c)|), and direction of the gradient (\theta(r,c) = arctan(G_r(r,c), G_c(r,c))). \n3. Non-maximal suppression to locate best edge positions (suppress lower gradient magnitude values = set to 0 values not on the ridge). Observe a 3x3 grid with the gradient direction. Suppress center pixel H if H < H_{\alpha} or H < H_{\beta}, where H_{\alpha} = AH_1 + (1 - A)H_2, H_{\beta} = AH_4 + (1 - A)H_3, and A = |G_r|/|G_c|. \n4. Hysteresis edge tracking to locate reliable but weak edges. Thresholding stage. Start edges at certainty \nH > \tau_{start}, continue at the ridge and reduce to H > \tau_{continue}.

• Could we use Gaussian smoothing to remove salt & pepper noise?
- No, use conservative smoothing (neighbourhood).
 
• What do we assume when finding straight lines with RANSAC? How does it work? How do we relate the failure probability to expected number of trials in RANSAC?
- Shape of the feature is determined by T true data points, and hypothesized feature is valid if V data points nearby.  \nIterate over n trials: select T data points randomly, estimate feature parameters and return success if the number of nearby data points > V. \nAlgorithm fails if Trials consecutive failures: p_{all - f} = (p_{one - f})^{Trials} \nSuccess if all needed T random data items are valid: p_{one - f} = 1 - p_1(p_d)^{T-1} where p_1 is the probability of a data point belonging to a valid feature, and p_f is the probability of a data point belonging to the same feature \nThen solving for expected number of trials: Trials = \frac{log(p_{all - f})}{log(1 - p_1(p_d)^{T - 1})}

• How do we find actual line segments after RANSAC?
- 1. Project points {x_i} (because of noise) onto ideal line through p with direction a: \lambda_i = (x_i - p)a, projected point is p + \lambda_ia. \n2. ('heuristic cleaning') Remove points not having 0.9\tau neighbour points within \tau = 130 pixels distance along line. \n3. Endpoints are given by smallest and largest remaining \lambda_i.

• How do we find left and right line pairs?
- Check each pair and: \n1. Reject if orientations are not similar (vector dot < 0.9). \n2. Compute 3D overlap between 2 images. Want to find corresponding edge points on 2 images lying on the 2 lines by 1) computing points on the left line that cross image, 2) computing prospective space representation of right image line, 3) computing point on the right line satisfying epipolar constraint for each point on the left line, 3) keep points if p_l and p_{rl} are near, 4) keep longest subset of consecutive valid edge points and recompute segment midpoints \n3. Reject short overlaps. \n4. Recompute segment midpoints given overlap. \n5. Compute contrast at midpoint. contrast = average(red pixels) - average(blue pixels), where red are the pixels perpendicular to line at midpoint to the right, and blue same but to the left \n6. Reject pairs that do not have similar contrasts and suitable disparity range. \n7. Remove pairs that are not unique.

• Since the two cameras can be at different positions and orientations, how can we tell when two portions of the lines overlap?
- If two points a and b in the left and right images correspond, then they must satisfy: b′Fa = 0

• What are some ways to constrain matches between left and right images in stereo matching?
- 1. Edge direction: match features with nearly same orientation. \n2. Edge contrast: match features with nearly same contrast across edge. \n3. Feature shape: match features with nearly same length. \n4. Smoothness: match features giving same depth as neighbors. \n5. Uniqueness: feature in one image can match from the other image 0 if occluded, 1, or +2 due to transparencies, wires, etc. \n6. Epipolar Geometry: match points satisfying p_re = 0 where e = Fp_l

• What is the key computational advantage of the epipolar constraint?
- Since potential matching feature can only lie along the epipolar line, this greatly reduces the number of potential matches.

• What is the key idea of recovering 3D positions from two matched lines? How exactly do we compute that?
- Want to find correspondences between left line and right line. Key idea: Compute 3D plane that goes through the image line and camera origin, then intersection of 3D planes from 2 cameras. \nTake midpoint of left line, project using F to the right image, then compute second point on the left line adding an offset in the direction of the line u and project again with F. Triangulate points to get 3D points: given projection and parameter matrices (Ps and Ks) compute algebraic quantities as vectors and compute svd on them. Read off 3D vector from columns of V. Finally compute matched line 3D midpoints as well as direction.

• What is the key geometric insight behind finding the 3D line segments?
- The principal point of the camera and the 2D line segment in the image defines a 3D plane. Intersecting the two planes backprojected from the left and right images gives a 3D line segment.

• How do we match 3D data edges to 3D wireframe model edges?
- Use Interpretation Tree algorithm with Limit = 5: \nUnary test: similar length |l_m - l_d| < \tau_l(l_m + l_d) (no effect cause mostly same length edges). \nBinary test: similar angle between pairs: |\theta_m - \theta_d| < \tau_a(=0.5)

• How do we do 3D Pose Estimation in stereo?
- Given matched line directions {(m_i, d_i)} and points on corresponding lines {(a_i, b_i)} compute: \nRotation R from matched vectors but use line directions instead of surface normals, try each direction for edge correspondence and if det(R) = -1 then need to flip symmetry. \nTranslation, given N paired model and data segments, with point a_i on model i and b_i on data i, direction d_i of data i and estimated R, find t that minimizes \sum_i \lambda_i'\lambda_i where \lambda_i = Ra_i + t - b - d_i(d_i'(Ra_i + t - b)), solution is t = L^{-1}n where L = \sum_i(I - d_id_i')'(I - d_id_i') and n = \sum_i(I - d_id_i')'(I - d_id_i')(Ra_i - b_i)

• How do we do 3D Match Verification in stereo?
- Like 2D but measure 3D quantities: \n1. Rotated 3D model line similar orientation to estimated 3D scene line. \n2. Rotated & translated model line endpoints near infinite 3D scene line. \n3. Rotate & translated model midpoint near estimated 3D scene line midpoint.

• How do we get depth data for every point when we only have some at triangulated feature locations? How does correlation-based stereo work?
- A simple way would be to linearly interpolate known values at all other pixels. However, correlation-based stereo works better. \nMatch features (neighborhoods at each pixel) from both images using SSD: SSD(u,v,r,s) = \sum_{i=-\frac{N}{2}}^{\frac{N}{2}}\sum_{j=-\frac{N}{2}}^{\frac{N}{2}}(L(u+i,v+j) - R(r+i, s+j))^2, find best match by building an array of possible match scores and search lowest cost path through dynamic programming.

6 Deformable Part Models

• Why do we use PCA? What does PCA do?
- Many reasons: \n1. It chooses axis directions a_i in order of largest remaining variation. \n2. Gives an ordering on dimensions from most to least significant. \n3. Allows us to omit low significance axes. \nGiven a set of D dimension points {x_i} with mean m, it finds a new set of D perpendicular coordinate axes {a_j} such that x_i = m + \sum_j{w_{ij}a_j}

• How do we find the weights of a transformation on x_i, if we are given x_i, a set of perpendicular axes, and the mean m?
- a_k \dot (x_i - m) = a_k \dot \sum_j w_{ij}a_j = \sum_j w_{ij}a_k \dot a_j = w_{ik}

• How do we compute PCA?
- There's 2 ways, the first is: \n1. Choose axis a_1 as the direction of the most variation in the dataset. \n2. Project each x_i onto a D - 1 dimensional subspace perpendicular to a_1 to give x_i'. \n3. Calculate the axis a_2 as the direction of the most remaining variation in {x_i'}. \n4. Project each x_i' onto a D - 2 dimension subspace. \n5. Continue until all D new axes a_i are found. \nThe second and most popular way (via eigenanalysis): Given N D-dimensional points {x_i}: \n1. Mean m = \frac{1}{N}\sum_ix_i \n2. Compute scatter matrix S = \sum_i(x_i - m)(x_i - m)' \n3. Compute SVD: svd(S) = UDV' where D is diagonal, U'U = V'V = I. \n4. PCA: i^{th} column of V is axis a_i (i^{th} eigenvector of S), d_{ii} of D is a measure of significance (i^{th} eigenvalue)

• What are Point Distribution Models? What main assumption do they make, and what are they used for?
- Models that given a set of objects from the same class, and a set of point positions {x_i} for each object instance, capture structural as well as statistical position variation. \nThey assume that point positions have a systematic structural variation plus a Gaussian noise point distribution. \nThey are used in recognition.

• How do Point Distribution Models work (formally)?
- Given N observations, each with P boundary points {(r_{ik}, c_{ik})}, k = 1...P, i = 1...N, rewrite {(r_{ik}, c_{ik})} as a new 2P vector x_i = (r_{i1}, c_{i1}, r_{i2}, c_{i2}, ..., r_{ip}, c_{ip})' and find correlated components with PCA (which will exist if there is systematic variation). PCA gives a set of 2P axes such that x_i = m + \sum_{j=1}^{2P}w_{ij}a_j, which we use a subset M of the most significant ones (replace 2P for M, based on eigenvalue size). \nThe structural model is now the representation of x_i as w_i = (w_{i1},...,w_{iM})' (which we can use to approximate original data with variation), and the statistical (Gaussian) model is the mean t = \frac{1}{N}\sum_i w_i, covariance C = \frac{1}{N-1}\sum_i (w_i - t)(w_i - t)'

• What different aspects of the objects do the structural and statistical models encode in PDMs?
- The structural model encodes the ways in which the individuals can vary and the statistical model encodes how much variation is normal.

• What preprocessing steps do we need to make for PDMs? Why is preprocessing that re-positions the parts needed before the PDM method is used? 
- Load image, convert to binary for most tasks. Then it becomes task-specific: \n1) Get boundary and find corners by exploiting problem-specific knowledge (like searching for intersecting lines in a badly segmented square) \n2) Get shapes in standard position, eg.: rotating TEE using a heuristic algorithm: \n1. Sort 8 lines into 2 sets of 4 mutually nearly parallel lines (reject if not possible): find direction of one line, sort all others by checking the angle with this line. \n2. Find which set is the head of TEE (reject if neither or both satisfy criteria) and sort into positional order: if longest is sufficiently longer than the next and the 3 shortest are about the same length as the longest, the longest is the head of TEE. \n3. Estimate transformation of TEE to standard position with TEE head top parallel to column axis and center of TEE at origin. Apply transformation to TEE. \nThe PCA analysis looks for how features vary relative to a standard configuration, so we need the parts to be in the standard configuration.

• What is the Mahalanobis Distance? What two complications does the Mahalanobis distance compensate for?
- Formaly, the same as Euclidean but with an embedded covariance matrix C (ie. |(a - b)'C^{-1}(a - b)|^{1/2}) \n1. The components of the vector might have difference ranges (e.g. component 1 has a range of -1000 to +1000, whereas component 2 might have the range -0.1 to +0.1) \n2. The components of the vector might be correlated, and the Mahalanobis distance de-correlates the components before computing the distance (e.g. if the first and second components of the vector were always identical, then this would bias the distance. The MD de-correlates them so they only count for 1 difference).

• How do we do PDM based classification/recognition? 
- Given unknown sample x, structural model (mean m and M variation axes a_j) and statistical model (class means {t_i} and associated covariance matrices {C_i} for i = 1..K classes) do for each class i: \n1. Project x onto a_j to get weights w (M dim vector) \n2. Compute Mahalanobis distances: d_i(w) = ((w-t_i)'(C_i)^{-1}(w-t_i))^{1/2} \nThen select class i with smallest distance d_i(w) and reject if smallest distance is too large 

• Give all the steps for PDMs model learning and recognition on the TEE dataset. What could the first few principal components encode for the TEE dataset?
- a) Load image, threshold, get boundary and find corners (using improved corner detection by using shape-specific knowledge). \nb) Reject if not 8 vertices, rotate TEE/TEEs to standard position and get vertices into vector in a standard order. \nc) Project example/sample into PCA weight space. \nFor Learning do: d) Estimate statistical model of projections \nFor Recognition do: d) Evaluate statistical likelihood of projection (statistical model) using Mahalanobis distance. \nWidth of the tee head, overall size of part, thickness of the tee head of tee stem, etc.

• What are some issues with PDM? What are some advantages?
- 3 main issues: \n1. Failures due to not finding enough corner points from segmentation. \n2. Finding the points but not correct ones. \n3. Having to align shapes heuristically. \nSome advantages include: \n1. Applicable to smooth curves and other data besides points (3D or ultrasound). \n2. Covariance matrix links correlations between pairs of features (tho could extend to triples?). \n3. Could extend to include greylevel variations. \n4. Training samples need not be in order.

• What is the main requirement on a dataset before we can use the PDM method?
- There must be the same number of points, corresponding to the same features of the object being modeled, and viewed in approximately the same spatial relationships.

• What is the key principle of the eigenface routine? How do we create the eigenfaces? How does Eigenfaces Recognition work? How well does it perform? What are some issues with Eigenfaces?
- Key principle: Turn image array into long vector as a weighted sum of eigenimages. \nLearning: Given a set of K registered face images (RxC) with varying capture conditions, do PCA on the RxC long vectors. Then represent person i by projecton weights w_t. \nRecognition: Given unkown face image F_u: \n1. Subtract mean face and project onto eigenfaces -> w_u. \n2. Given database of projections {w_i}_{i=1}^K, find class c with smallest Mahalanobis distance d_c to w_u. \n3. If d_c small enough, return c as identity. \nPerformance: Successful over lighting variations (96%), good over orientation variations (85%), and alright over size variations (64%). \nIssues: Variations in position, orientations, scale and occlusion cause problems and it has 4-36% failure rate, which is not great for say busy airports.

• What are the correspondences used for the eigenface method? What assumption is required to ensure that the correspondences are valid?
- It assumes that the images are perfectly aligned so that the pixels map to corresponding points on people’s faces. This means that the systematic variations are in face illumination, not face shape.

7 Persistent Tracking and Behaviour recognition

• What next steps can automatic systems take after targets are detected and tracked? 
- 1) One can link together tracks broken by occlusion and other effects. 2) One can identify what people are doing at any particular instant (and also combine these to identify medium and longer-term activities).

• What is the general problem of target tracking? What is the main issue and how to solve it? How do we represent the tracking problem?
- Pairing R targets {F_i} in one frame to L targets {N_i} in the next one. \nThe issue is that tracking targets breaks down when close or occluded. To solve it we need identity persistence through occlusion. \nBreak trajectories at occlusions. Create label nodes X_i for each track S_i, also inheriting previous tracks

• How do we match colour targets?
- Assuming we have a binary mask of the target that we can use to select the pixels: Compute RGB histogram from those pixels \forall i h_1(r_i,g_i,b_i) = h_1(r_i,g_i,b_i) + 1 and match with h_2(r_i,g_i,b_i). \nIn order to match we compute the Bhattacharrya distance between normalized histograms H_j(r,g,b) = h_j(r,g,b) / \sum_{r,g,b} h_j(r,g,b) as d(H_1, H_2) = -ln(\sum_{r,g,b} \sqrt{H_1(r,g,b) x H_2(r,g,b)}). If d(h_1,h_2) is small enough then likely the same target, also group colour levels together into blocks.

• These Bayesian network graphs do not have any loops. How could we handle a person walking in a loop, meeting the same person several times?
- A looping behaviour does not mean a looping graph - rather it means repeated node structure at later times in the graph

• What extensions are needed in the Bayesian Network? Given these, how do we then evaluate persistence?
- Create label nodes Xi for each segment section Si. \nAdd data matching nodes Yi (color histogram of target appearance). \nAdd restriction nodes Ri enforcing mutual exclusion between sibling nodes. \nTo evaluate persistence use a standard conditional probability propagation algorithm to find labeling X that maximizes p(X|Y,R). That is the probability of labeling X given data Y and restrictions R, which gives gives probability that each person Pi is observed in track Xj.

• How do we deal with the computational expense of evaluating the full network? What about the growth of the network?
- Incremental evaluation, using Bayes rule after the kth block of T frames: \np(x_i|Y_0^t, R_0^t) = \alpha p(Y_{kT}^t, R_{kT}^t|x_i)p(x_i|Y_0^{kT}, R_0^{kT}) \nCould freeze all but N most recent nodes. That is to fix most probable identity for track Si rather than keep all possible ids (which could change with future evidence).

• What characterises the near, medium and far field tracked targets?
- Near field means that the object is close enough that you can see details of body parts and pose. Far field means that the object is so far that you cannot make out any details - it’s just a moving blob. Medium field is in between, where you can make out some colour and rough shape, but not details of body configuration, pose or identity

• What 3 processes are required to obtain a description of actions?
- Stabilise moving object, compute optical flow and compute descriptors based on the optical flow.

• How do we stabilize images?
- We want to threshold temporal difference (subtraction of consecutive frames) for regions of interest (want a measure of how boxes should overlap over each other). To do that use maximum cross-correlation cc(dc,dr) between the image boxes to estimate frame-to-frame translation (dc,dr) (offset to stabilize image). 

• How are the optical flow descriptors computed? What are its advantages and disadvantages?
- Given the optical flow components (u,v) (where each pixel's data is moving to in next image, from an algorithm matching local to temporal gradients) split +/- components as (u_i,v_i) -> (f(u_i),f(-u_i),f(v_i),f(-v_i)) and smooth (Gaussian conv) independently. Gives a_c^i(x,y) for each frame i, OF component c and pixel positions (x,y). \nIt is rich and subtle but view/scale dependent.

• Why must we stabilize the image before computing the optical flow?
- We are interested in the motion of the person relative to him/herself (which partially defines the action) rather than how the person moves against the background. Stabilisation works here (football player) because the background is largely constant green grass. Alternatively, one could detect the person against a fixed background model and then replace the background by a constant intensity.

• How do we match OF descriptors?
- If we wanted to match a single frame use m(i,j) = \sum_{c=1}^4 \sum_{x,y \in L} a_c^i(x,y) b_c^j(x,y). However usually we want to match consecutive frames through a weighted sum of nearby in time frames (eg. T = 50 frames) as S(i,j) = \sum_{r=-T/2}^{r=+T/2}\sum_{s=-T/2}^{s=+T/2} K(r,s)m(i+r,j+s)

• What is the main technique for assessing the similarity of two video frames?
- The optical flow measured at the 2 individual frames is compared. The similarity score is computed by combining the frame’s positive and negative U and V optical flow components at each pixel in the bounding box.

8 Deep Nets for Vision

• What are three typical units in a Deep Net for Vision?
- Convolution: Out_{i,k} = \sum_{j \in ne(i)} weight_{j,k} x Input_j \nReLU: Out_i = max(0, Input_i) \nMax-Pooling: Out_i = max_{j \in ne(i)} Input_j

• What are some typical image pre-processing steps?
- Resizing images to network size (eg. 224*224) or normalising image values. Could normalise each image independently or together. If together it would be as follows: \n1. Calculate mean m and standard deviation \sigma over all images, pixel values and color channels. \n2. For image i, channel c \in {R,G,B} and image pixel p: Val_{i,p,c} = (Val_{i,p,c} - m) / \sigma

• What are some typical training steps?
- Split training (3/5), validation (1/5) and test (1/5) sets. \nUse cross-validation (eg. 5-fold and average performance from 5 test sets). \nUse large image databases (eg. ImageNet). \nUse a GPU for paralellising computations.

• Why and how do we use pre-trained networks?
- Because initial layers are generic, and some datasets (ImageNet) are large making it very expensive to train. \nWe can use generic convolution layers, replace fully connected ones, and retrain with target image set. (eg. AlexNet, Inception, ResNet, VGGNet )

• Where do you find the ‘features’ that can be used from a deep net? Potentially how many are there in the AlexNet architecture?
- They are typically the values from the final layer of the convolutional part of the deep net, i.e. the values that go into the fully connected layers. AlexNet has 13*13 wide and 256 layers in the final block, so potentially 13*13*256 = 43264 features (many of which are probably irrelevant)

• What are the details of AlexNet?
- Split into 2 halves due to GPU limitations, 5 convolution layers followed by 2 fully connected and a 1000 class SoftMax. Some (slightly overlapping) maxpooling and ReLU layers. \nIt uses local response normalization, normalizing this way helps emphasize cells responding well to the data. \nNeeds data augmentation, such as translating and flipping (reflecting), as well as adding random Gaussian noise. \nUses dropout, regularization with weight decay and 128 sample batches.

• How did the authors of AlexNet do the similarity matching?
- They compared each image's final descriptor vector (input to the softmax) to each other. 

• What sort of patterns appear in the first layers of AlexNet?
- There are a number of patterns. If we focus on the Red/Green patterns, we see a red blob and a green blob. There are red:green patches at 4 orientations. These are like color edge detectors. There are black:white patches at 4 orientations. These are like intensity edge detectors. There are white:black:white and black:white:black patches at c. 10 orientations each. These are bar detectors. These are some of the patterns that primate eye & brain neurons respond to.

• How might you set up and train a deep net to do iris-based person recognition?
- 1) Start with something like AlexNet, pretrained on ImageNet. This would give a set of features. 2) Replace the final output layer with 1 cell per person in the database. 3) Re-initialise and retrain the fully-connected layers with a set of iris samples (hopefully multiple samples per eye under different lighting conditions and from different days)

• Why are the adversarial and nearby wrong classifications a problem?
- It means that the network has not generalized well. Adversarial images look correct but are incorrectly classified, even though they are only slightly different from the original images. Similarly, nearby images are only slightly different. We certainly want visually similar images to be correctly classified. Moreover, we also want images that have never been used in training to be correctly classified.

• How can we visualize classifications for data in the form of 1000-D feature vectors?
- Could use a dimensionality reduction technique such as t-SNE. If features clearly give a distinct representaion of the data it will give a clear boundary (0/1 vs cars/cats). t-SNE could also be used to identify adversaries, we can devise one by progressively adding a random vector to a point until it reverses the classification (classification bubble).



