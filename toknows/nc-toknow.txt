To Know List
------------

1 Important concepts

• What is the aim, and approach of Neural Computation? What kinds of models are there?
- Describing the development and function of the brain in mathematical and computational terms. In order to do so, we attempt to isolate the key factors contributing to observed phenomena. \nDescriptive models: compact summary of experimental findings, e.g. curve fitting. \nMechanistic models: modelling the underlying mechanics, e.g. biophysical models. \nInterpretive models: formalise a high level hypothesis about the data, using e.g. information theory.

• State the main parts of the brain and function. What happens when each main part is damaged?
- 1) The back (occipital area - visual processing): blindness, colour vision, motion detection. \n2) Medial part: on the side there could be deficits in object recognition or language. On top (somato-sensory input) issues with motor planning and motor output. \n3) Frontal part: short term or working memory, planning and integration of thought, frontal lobotomy leaves people like zombies.

• What are the functions of the cerebellum and the hippocampus?
- Cerebellum: precise timing functions, specifically motor functions (patients can't dance). \nHippocampus: Processes and associates temporal information ('buffer'). Absence leads to new memories not being stored anymore. Could be the origin of severe epilepsy.

• What are glial cells? Describe the anatomy of neurons and how they communicate. What kinds of neurons are there?
- Glial cells provide support to the neurons, like myelin sheets around axons or sucking the split over neurotransmitters. \nNeurons are composed of a nucleus (inside the soma), mitochondria and other organelles. They receive inputs through synapses on their dendrites, in the form of spikes. These are brief excursions of the neuron's membrane voltage. Myelin sheets allow the speed increase of signal propagation. Neurons can be excitatory or inhibitory, depending on the kind of neurotransmitter they release.

• Indicate the time and spatial resolution of measuring techniques, as well as the signal used and whether it stimulates neurons or not.
- Intracellular electrode: bad, single neuron, measures currents using patch clamp, stimulates. \nExtracellular electrode: bad, single neuron, picks up spikes after stimulation, although imprecise. \nEEG: good, large areas, measures potential on the scalp (sum of many extracellular potentials), no stimulation. \nfMRI: good, 1-5mm area, measures increased blood oxygenation levels, no stimulation. \nOptical signal: good, single neurons, measures activity from fluorescent indicators (dyes sensitive to Ca).

• What preparations have been developed to measure neurons?
- in vivo: without anaesthesia is problematic, with the recordings will be unreliable. Anaesthesia changes the functioning of the nervous system. \nin vitro: 1/2 mm thick brain slices. Could be severed, reproducibility is not clear. \nculturing: cells from young brain, unclear relevance with in vivo.

2 Passive properties of neurons

• Derive the Nerst equation
- [1.1] 

• Give all the equations of the passive neuron model
- [1.2] 

• Why do we need to model the neuron as a cable, and how? Give the cable equation and draw/explain all the components needed.
- Due to its extended structures, such as dendrites and axons which are not perfectly coupled. The idea is to couple many cylindrical compartments with each other with resistors. [1.3]

• Give the steady-state and time-dependent solutions to the passive cable equation.
- [1.3.1], [1.3.2]

3 Hogkin-Huxley model

• How do Action Potentials (AP)/Spikes arise in neurons?
- When injecting current into a neuron at rest (-70mV) the membrane voltage will start to rise. When it reaches a threshold voltage (-50mV), it will rapidly depolarise to about +10mV and then hyper-polarise to about -70mV. The spike travels down the axon, causing the axon-terminals to release neurotransmitters which excites or inhibits the next neuron in the pathway.

• What does the Hogkin-Huxley model say about how spikes arise?
- Na channels begin opening when membrane voltage is close to the threshold, and since the concentration outside is higher it starts flowing in depolarising the cell. This triggers more channels to open and initiate a spike. Right after the spike, Na channels close and K channels open, triggering K ions to flow out hyper-polarising the cell back to resting potential.

• Describe repetitive firing. What does the FI-curve show and what affects it?
- When the HH model is stimulated for a longer time, multiple spikes in succession can be produced. The FI-curve shows the firing frequency vs input current. Other channel types and the cell geometry can have an inpact on the FI-cuve, as well as noise that could potentially smooth it.

• Briefly describe the other channels found in neurons.
- The KA current (I_A) inactivates at higher voltages and can delay spiking. \nThe I_H current only opens at hyper-polarised membrane voltages, thus prevents prolonged hyper-polarisation. \nCalcium channels have many uses, including neurotransmitter release and signal amplification in dentrites. Ca^{2+} activates other channels, sends signals for plasticity and does housekeeping/homeostasis jobs. \nKCA cause spike-frequency adaptation, which causes the spike to become slower or stop when internal Ca concentration builds up. 

• How are Ion channels distributed?
- Ca channels are found at the synapses, and dense Na channels at the axon hillook (where spikes initiate), as well as everywhere else.

• What are some challenges in computational models?
- Often precise quantities for the channel distributions or types are not known, although it can be shown in simulations that certain parameters have little influence. This illustrates the trade-off between detail and tractability.

4 Synaptic Input

• What are synapses? What are vesicles?
- They are the connection points between neurons. They are responsible for most computations, including learning and memory. \nVesicles are spherical structures containing neurotransmitters.

• How does synaptic transmission occur?
- At the axon terminal there are two groups of vesicles: "reserve" pool and releasable vesicles. The former is inside the cytoplasm and the latter are on the wall of the membranes awaiting a signal to be released alongside Ca channels. At the postsynaptic cell there are neurotransmitter receptors, proteins where neurotransmitter can bind, and leads to the opening of a channel. \nWhen an action potential comes in, Ca channels de-polarise the membrane, allowing Ca to flow in and bind to the release machinery. This incites the vesicles to release neurotransmitters outside the axon, and is captured by the receptors at the postsynaptic cell creating a postsynaptic current. Empty vesicles return to the pool, while being replaced by vesicles from the pool.

• Is it possible to simulate transmitter diffusion?
- Yes, using Monte Carlo simulation. It simulates each neurotransmitter molecule, gathering movement, concentration gradients, etc. It can simulate the post-synaptic current from a single release event.
 
• What are the various types of neurotransmitter receptors?
- Ionotropic receptors: have an ion channel built in and will open when neurotransmitter binds to them. They can be split by excitatory (depolarising ion:K, Na, Ca) receptors, whose main binding neurotransmitters are glutamate (AMPA, NMDA) and acetylcholine (nicotine, nACh); and inhibitory (Cl) receptors like GABA(A,C), Glycine. \nMetabotropic: trigger a 2nd messenger cascade, might open a channel. Slower than ionotropic receptors, same subdivisions. Other neurotransmitters include serotonin, dopamine, various hormones, endocannabinoids, etc. Psychoactive drugs incite a cell state change rather than immediate transmission.

• Do neurons release more than one transmitter?
- While neurons can have many types of receptors, they can only release one transmitter.

• How do we model the AMPA receptor?
- [3.1]

• Give the formal definition of kinetic models.
- [3.2] 

• Can we simplify kinetic models?
- Yes, computing receptor activation using Markov diagrams can be computationally expensive so we can approximate the model in two ways: a) having an exponential decay; b) alpha function [3.2.1].  

• How does the NMDA receptor work?
- NMDA current depends non-linearly on the voltage due to Mg block. At voltages close to resting potential, Mg ion blocks the pore of NMDA channels. Current can then only flow at higher voltages. Additionally, Glu unbinds much more slowly than in AMPA. The Mg block can be fitted to [3.3].

• How do we measure the strength of a synapse?
- Miniature Excitatory Post Synaptic Currents (mEPSC) or mIPSC (for inhibitory synapses). The number of vesicles released per spike k is given by the Poisson distribution or the Binomial (if the number of release sites is limited) [3.4].

• How can we approximate the statistics of channel opening?
- Using binomial statistics [3.5].

• What is and when is short-term plasticity observable?
- Synaptic transmission does not stay constant, is subject to the recent history (however not permanent either). This can be modelled by synaptic depression and facilitation [3.6]. It can be observed when we stimulate a synapse repeatedly with the same stimulus. 

• How do we model the conductance of many synapses?
- [3.7]

5 The integrate and fire model

• What is special about the I&F model?
- We replace the spike generating mechanism by a simple thresholding mechanism. Details in [4.1].

• How can we adapt the spiking frequency in the I&F model to include Ca concentration?
- [4.3]

• What are the ways to model synaptic input in I&F models?
- [4.4]

• How do we combine inhibitory and excitatory conductances? What is shunting inhibition?
- [4.5]. Inhibition does not hyper-polarise the neuron but forces the membrane potential toward the resting potential (because inhibitory reversal potential V_{inh} is about resting potential).

• What happens for strong inhibition (g_i >> g_e, g_l)?
- The difference between resting potential and steady state is [4.5.1]. Thus the effect of inhibition is divisive on V_m.

6 Single neuron firing statistics and noise

• What 6 interval statistics can we obtain?
- [5.1]

• What does the coefficient of variation (CoV) tell us?
- It allows us to pinpoint what the underlying distribution is. This is because the CoV has known values for various distributions.

• Give the 4 quantities from the Poisson model for spike generation.
- [5.2]

• What are the expected values for the CoV?
- For a Poisson process it should be close to 1. It is smaller than one, corresponds to regular firing. Anything less regular than random should be larger than 1 (rate fluctuates over time).

• What is the Fano factor?
- Value derived from counting spikes and computing statistics [5.3]. Fano factor of 1 suggests a Poisson process (like CoV).

• Give 5 reasons for variability of in vivo neural activity (~3000Hz argument).
- 1. Excitatory input is balanced by inhibitory inputs, reducing net drive (time constant) and increasing noise (membrane leak). \n2. Morphology may contribute (geometry and active properties). \n3. Cortical rater are likely to be lower than 3Hz (activity may be sparser). \n4. Input strength varies (weighting of different inputs). \n5. Input may not be accurately described by uncorrelated processes. The presence of correlations reduce the number of independent inputs and so firing is more irregular.

7 A visual processing task: Retina and V1

• What is the retina? Do photoreceptors spike? What neurons spike in the retina?
- It is the place where light becomes neural activity (electricity out of images). Photoreceptors do not spike, since they don't have sodium channels. Spiking neurons are the ganglion cells.

• What types of retinal ganglion cells are there?
- There are ON and OFF cells. ON cells respond maximally to a white spot on a black background and OFF viceversa. Humans don't have many but there are also ON-OFF cells which react to light of both intensities.

• How can we model the receptive fields of retinal ganglion cells?
- Using a difference of Gaussians (DOG), similarly to an edge-detection computer vision system.

• What is neural adaptation?
- Matching a transfer function with the statistics of an input. The idea is to change the input/output relation of a cell such that its dynamic range is optimally used.

• What happens with the information decoded in the retina?
- Activity produced in the retina in response to light stimuli is transmitted through the optic tract over into the primary visual cortex (V1). Most of the information the left eye receives is transmitted to the right hemisphere of the brain and viceversa (contralateral response, as opposed to ipsilateral). The mix of signals from ipsi and contra-lateral sides allows us to do stereo vision.

• What is the purpose of the fovea?
- It's a neuronally dense area (>1000 neurons than periphery) used for perceiving objects in focus at high resolution. Additionally, places in the visual field correlate with the locations of firing neurons.

• What are the information pathways from the retina to V1?
- 3 types of cells: \nParasol: larger receptive fields, no fine spatial details. \nMidget: no spatial averaging (fovea makes contact with single photoreceptor), fine spatial resolution. \nBistratified: unclear, maybe motion sensitivity. \nAll three travel through the LGN (relay) and end at different parts of the cortex. The pathways are called Parvocellular (Bistratified - object recognition), and Magnocellular (Parasol - localising objects, motion sensitivity).

• What kinds of cell selectivity and selective cells are there?
-  Direction and Orientation selectivity. Orientation selectivity is distributed in cortical columns over the cortical surface, and it spans the whole range of possible orientation stimuli. Simple cells respond to bars/edges at a preferred orientation, complex cells too but response is position-invariant.

• How do we formalise receptive fields?
- Using Linear-nonlinear models as in [6.1].

• How can we measure receptive fields?
- Through reverse-correlation. First, stimulate simple cells with white noise and record spikes as well as the stimulus. Then collect and average all successful stimulus.

8 Higher Visual Processing

• How does the retina do object recognition?
- By processing snapshots of objects in a scene every 200ms.

• What are the two main streams for higher visual processing in the visual cortex?
- Ventral, which correlates to form recognition and object representation; and dorsal, correlated with motion, location and control of eyes and arms.

• What sort of responses do we find in the 6 components of the ventral pathway? What about in MT, the main component of the dorsal pathway?
- 1. Retina/LGN: luminance, contrast. \n2. V1: orientation. \n3. V2: figures, contours. \n4. V4: geometrically complex objects. \n5. PIT: complex objects, parts of objects. \n6. AIT: known objects, categories. \nMT: motion, studied using random dot patterns.

• What is the feedforward 'sweep'?
- After presenting an image, what we call the 'sweep' is the successive hierarchical levels of the visual cortex which are rapidly activated through the cascade of feedforward connections. It allows object recognition in 150ms, one or few spikes per stage.

• How do we figure out what higher visual neurons do?
- Place an electrode in the area and show objects, record response and find out what activates the area. Then simplify the objects.

• What is special about TE?
- They have large receptive fields, including the fovea and some cells responding to complex shapes. It requires a combination of features for representation.

• Does the higher visual area also organise itself in columns?
- Yes, columns of neurons are selective for the same objects. Additionally, TE neurons represent global features as well as local features, such as spatial relationship among object parts.

• Suppose we have N neurons, k of them are activated by a stimulus. What sort of schemes can we use to encode that?
- k=1: Local codes or Grandmother cells. What's good is that it can represent N stimuli at once, and computations are easy. The problems are low coding capacity as there are only N outputs, and it's hard to generalize between stimuli. Examples include the Jennifer Aniston Neuron (extremely selective neuron). \nk=~0.5N: Dense codes. It's good because highest capacity is 2^N, but it can only represent one stimulus at a time. Also it can be hard to extract information and consumes more energy. \n1<<k<<N: Sparse code. Consensus, can represent a few stimuli at once, and help with generalisation.

• What is sparseness, and how do we measure it?
- The number of neurons activated by one object. From a histogram of activations (spikes/s) recorded after showing objects we can measure sparseness (peakedness of distribution) by: \n1. If we consider neurons to be active or inactive: average fraction of active neurons. \n2. For continuous activity: [7.1]

• What are the 4 principles of brain codes?
- 1. Codes are sparse and distributed. \n2. Neurons represent some features consistently. \n3. Nearby neurons have typically similar tuning properties. \n4. Codes are 'efficient' for the task at hand.

• What do intracellular and multi-unit extracellular recordings tell us? How else could we look at connectivity and computation?
- Intracellular tells us about the inputs to the neurons, and multi-unit extracellular tells us how neurons are connected to each other. Could look at anatomy, although very hard; or modelling, like using convolutional networks.

• Are there other types of connections (not just feedforward)?
- Yes, apart from feedforward there are also recurrent (lateral) and feedback. We call this top-down processing, and conveys contextual information, attention or expectations. We can consider those feedback sweeps as predictions (unconscious inference).

• What is the role of plasticity in higher visual areas?
- Neural responses change with familiarity, they signal associations between learned stimuli, and perceptual learning.

• What is the main challenge that makes brain >> machine?
- Invariance.

9 Neural Coding and Decoding

• What are the main two approaches of recording neural responses?
- Describing spike sequence, or number of spikes/rate in time window. Most of current work is concerned with the latter, and we also model the variability (variance, fano factor, etc.).

• How do we measure responses from single cells as opposed to populations?
- For obtaining the single cell tuning curve we change stimulus, and record spike count for every stimulus. For population response we keep the stimulus fixed, and then record spike count for every neuron in the population.

• What is the simplest decoding strategy?
- Winner take all: choose preferred orientation of the neuron that responds most

• What decoding strategy do we usually use?
- A population code, the sum of the rate times the orientation (in degrees) of each neuron. 
Population vector

• How can we decode neurons when we only know the encoding model (P[r|s])?
- Through maximum likelihood, choose the stimulus s that has maximal probability of having generated the observed response r (s = argmax_s P(r|s)). 

• How can we decode neurons when we know the encoding model (P[r|s]) and the prior on s (P[s])?
- Maximum a Posteriori, choose the stimulus s that is most likely, given r (s = argmax_s P(s|r) = argmax_s P[r|s]P[s]).

• How do we measure the performance of a decoder?
- [8.1]

• What is the Fisher information?
- [8.2] 

• What questions can we explore using the Fisher information?
- Factors that control performance, such as sharpening of tuning curve, gain modulation (adaptation, attention, perceptual learning), etc.

• What is Information Theory? What can we use Mutual Information for?
- A mathematical theory (based on probability) that describes the process of transmission of information over noisy channels. We can use Mutual Information to measure the information in a signal without making assumptions about the nature of the code.

• What are the main quantities of Information Theory?
- 1. Information content: how suprising is the realisation of one random variable (information gained when rv is sampled, = -log_2 P). \n2. Entropy: variability of one random variable. Average of information content (H = -\sum_i p_i log_2(p_i), the richer its response the more capacity for coding). \n3. Mutual information: measures the dependence between two random variables. It is the difference between response entropy and noise entropy, written as the KL-divergence between p(r, s) and p(r)p(s) [8.3].

• What is the idea of binding by synchrony?
- That maybe all the neurons participating in the representation of the same object tend to fire together. Synchrony could provide a label to neurons belonging to the same assembly.

• Describe cross-correlation (measuring synchrony)
- Measures the distance in time between the spikes of one neuron and the spikes of another (consistent delay between spikes).

10 Networks of Neurons

• Why would we and why wouldn't we model a network using spiking neurons (i&f)?
- Because of its comparison with electrophysiology, all neurons can be recorded at all times. Difficulties include having to set lots of parameters and assumptions, long simulations and analysing can be difficult.

• What are the ways to model a network of neurons?
- 1. Spiking neurons, eg. (i&f). \n2. Describe rate of spiking r(t) [9.1].

• Describe the Recurrent/Ring Model of orientation selectivity.
- It's a model of N neurons with preferred angle \theta_i, evenly distributed between -\pi/2 and \pi/2. All neurons receive thalamic inputs h and recurrent connections, with excitatory weights between nearby cells and inhibitory weights between cells that are further apart (mexican-hat profile).

•  What does the ring model achieve? Does it break symmetry in the input?
- Orientation selectivity and contrast invariance. Yes.

• What are attractor networks?
- A network of neurons, usually recurrent, whose time dynamics settle to a stable pattern. The pattern may be stationary (fixed-points), time-varying (cyclic) or stochastic-looking (chaotic).

• What is the crucial part of the brain for working memory? How could working memory be described?
- The prefrontal cortex. Working memory is a dynamic process that has not yielded to molecular characterisation, or also sustained activity. 

• What are Hopfield Networks?
- A form of attractor recurrent ANNs, with binary threshold units, symmetric weights. Can describe an "energy" in terms of the state of the units and weights. The way it works is that at each step a node at random is updated, guaranteeing the energy to go down settling in local minima. The weights are set to shape the local minima, and the network will learn to converge to stored states. It can also be an associative/content addressable memory, which means it can be used to recover from a distorted input the trained state most similar to that input.

• What is sustained activity?
- It's the principle that if recurrent connections are strong enough, the pattern of population activity once established can become independent of the structure of the input. It persists when input is removed. Could be a model for working memory.

11 Making Decisions and Motor Output

• What is signal detection theory (SDT)?
- Mathematical technique that detects signals against a background of noise. Internal responses can be described by a probability distribution, often approximated as a Gaussian, and often overlap.

• What affects discriminability?
- The distance between distributions (depends on the strength of the signal) and the width of the Gaussians (noise).

• What is a Receiver Operating Curve (ROC)?
- It is a graph describing the full range of the subject's opinion in a single curve, as the criterion is moved lower or higher (Hit rate vs False Alarm rate).

• What are the components of statistical inference?
- Selecting from hypotheses h1, h2, we have priors P(h), evidence e, likelihood P(e | h), and value v. We can then compare the posteriors P(h|e) for h1 and h2 using the likelihood ratio tests [10.1].

• How do we interpret the values?
- If h1 and h2, the benefit of choosing h1 is equal to the value of choosing h1 if h1 is true + the value of choosing h1 if h1 is wrong. Then we can rewrite the LR as [10.1].

• What are the components of the drift diffusion model of decision making?
- The decision variable (cumulated sum of evidence), the bounds, which represent the stopping rule, and a stochastic differential equation, Wiener process or Ornstein Uhlenbeck [10.2]. 

• What are the advantages of the drift diffusion model?
- Well studied mathematically, many variants, compared systematically and shown to be successful.

12 Models of associative memory

• How do we define and categorize learning and memory?
- Learning is an adaptive change in behaviour caused by experience, while memory is the storage and recall of previous experiences. Learning can be nonassociative or associative, depending whether one stimulus changes the whole system, or two stimuli are associated. Memory could be short term if its seconds to minutes, or long-term for hours to lifetime. Long-term memories could be declarative (recollection), nondeclarative (semantic)

• How do we form memories?
- Information is sent from neurons in the hippocampus to neurons in the cortex. With time, as connections between hippocampus and cortex degrade, connections between those same neurons in the cortex are reinforced.

• What is the idea of heteroassociation, and what kind of network can it be?
- To associate two different patterns of activity with each other, more specifically relate patterns of neurons firing to stimuli. Feedforward.

• What is the idea of autoassociation, and what kind of network can it be?
- To store patterns so that they can be recalled upon presenting fragments of it. Recurrent.

• What is information when talking about information efficacy of the associative net?
- Is is the number of bits of storage needed.

• What are palimpsest models, and what do they suggest?
- In palimpsest models we can keep putting new memories into the network because older ones are forgotten. These models suggest that the probability of recalling a memory is an exponential function.



