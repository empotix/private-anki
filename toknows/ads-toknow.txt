To Know List
------------

1 The Basics

• Define the worst-case running time:
- The (worst-case) running time of an algorithm A is the function T_A : N → N where T_A(n) is the maximum number of computation steps performed by A on an input of size n.

• Define the average-case running time:
- The average-case running time of an algorithm A is the function AVT_A : N → N where AVT_A(n) is the average number of computation steps performed by A on an input of size n. \nFor a genuine average–case analysis we need to know for each n the probability with which each input turns up. Usually we assume that all inputs of size n are equally likely.

• Define the concepts of Upper, Average-case and Lower bound:
- Given a problem, a function T(n) is an Upper Bound if there is an algorithm which solves the problem and has worst-case running time at most T(n); average-case bound if there is an algorithm which solves the problem and has average-case running time at most T(n); and lower bound if every algorithm which solves the problem must use at least T(n) time on some instance of size n for infinitely many n. 

• Let g : N → R. Define O, Ω and Θ notation:
- O-notation: O(g) is the set of all functions f : N → R for which there are constants c > 0 and n_0 ≥ 0 such that 0 ≤ f(n) ≤ c · g(n), for all n ≥ n_0. It reads as “Rate of change of f(n) is at most that of g(n)”. \nΩ-notation: Ω(g) is the set of all functions f : N → R for which there are constants c > 0 and n_0 ≥ 0 such that 0 ≤ c · g(n) ≤ f(n), for all n ≥ n_0. It reads as “Rate of change of f(n) is at least that of g(n)”. \nΘ-notation: Θ(g) is the set of all functions f : N → R for which there are constants c_1, c_2 > 0 and n_0 ≥ 0 such that 0 ≤ c_1 · g(n) ≤ f(n) ≤ c_2 · g(n), for all n ≥ n_0. It reads as “Rate of change of f (n) and g(n) are about the same”.

• What are the 3 methods to solve recurrences?
- Induction: Guess the solution and verify by induction on n. Lovely if your recurrence is “NICE” enough that you can guess-and-verify. Rare. \nUnfold and sum: “Unfold” the recurrence by iterated substitution on the “neat” values of n (often power of 2), and evaluate the sum that arises from the pattern. Must then verify the solution (e.g., by a direct induction proof).\n“Master Theorem” Match the recurrence against a template. Read off the solution from the Master Theorem.

• Lay out the Master Theorem:
- Let n_0 ∈ N, k ∈ N_0 and a, b ∈ R with a > 0 and b > 1, and let T : N → R satisfy the following recurrence: \nT(n) = Θ(1) if n < n_0, \nT(n) = a · T(n/b) + Θ(n^k) if n ≥ n_0.\n\nLet c = log_b(a); we call c the critical exponent. Then \nT(n) = Θ(n^c) if k < c (I), \nT(n) = Θ(n^c· lg(n)) if k = c (II), \nT(n) = Θ(n^k) if k > c (III).

2 Strassen's Algorithm and DFTs

• What is the Matrix Multiplication Problem?
- The product of two (n × n)-matrices A = (a_{ij})_{1≤i,j≤n} and B = (b_{ij})_{1≤i,j≤n} is the (n × n)-matrix C = AB where C = (c_{ij})_{1≤i,j≤n} with entries c_{ij} = \sum{k=1}{n}{a_{ik}b_{kj}}. \nThe MM Problem is the following: given as input two (n × n)-matrices A and B, output the (n × n)-matrix AB.

• What is a Discrete Fourier Transform (DFT)?
- The Discrete Fourier Transform (DFT) of a polynomial p(x) = a_0 + a_1x + . . . a_{n−1}x ^{n−1} of degree AT MOST n−1 is defined to be the point-value representation obtained by evaluating p(x) at each of the n-th roots of unity. The DFT is written as the list of values p(1), p(ω), p(ω^2), . . . , p(ω^{n−1}), where ω^n is any primitive nth root of unity.\nAnother definition: The Discrete Fourier Transform (DFT) of a sequence of n complex numbers a_0, a_1, a_2,. . . , a_{n−1} is defined to be the sequence of n complex numbers A(1), A(ω_n), A(ω^2_n), . . . , A(ω^{n−1}_n ) obtained by evaluating the polynomial A(x) = a_0 + a_1(x) + a_2x^2 + . . . + a_{n−1}x^{n−1} at each of the nth roots of unity.

• Lay out the Fundamental Theorem of Algebra:
- Let p(x) be any polynomial of degree n − 1 over the complex numbers (that is, let p(x) = a_0 + a_1x + . . . a_{n−1}x^{n−1}, for a_0, . . . , a_{n−1} ∈ C and a_{n−1} /= 0). Then there exist roots α_1, . . . , α_{n−1} ∈ C such that p(x) = a_{n−1}(x − α_1). . .(x − α_n). That is, the polynomial p(x) has exactly n−1 roots (counting multiple occurrences of the same root).

• What does the Interpolation Theorem say?
- For any set {(x_0, y_0),(x_1, y_1), . . . ,(x_{n−1}, y_{n−1})} of n point-value pairs such that the x_j values are all different, there is a unique polynomial A(x) of degree at most n − 1 such that y_j = A(x_j) for all j = 0, 1, . . . , n − 1. Moreover it is possible to compute A(x) from the point-value representation.

• What additional property do the complex numbers have?
- For every element a + ib ∈ C, and for every natural number k, there are exactly k solutions for the k-th root \sqrt{k}{a + ib} in the set C.

3 Sorting Algorithms and Dynamic Programming

• What is the definition of a comparison-based sorting algorithm? What are some examples of these?
- A sorting algorithm is comparison based if comparisons A[i] < A[j], A[i] ≤ A[j], A[i] = A[j], A[i] ≥  [j], A[i] > A[j] are the only ways in which it accesses the input elements. \n Some examples include: Insertion-Sort, Quicksort, Merge-Sort, Heapsort

• What is the concept of stability in sorting algorithms?
- A sorting algorithm is stable if it always leaves elements with equal keys in their original order.\n For example: Counting-Sort, Merge-Sort, and Insertion Sort are all stable. Quicksort is not stable.

• How does Dynamic Programming compare to the other two algorithmic paradigms, Divide and Conquer and Greedy Algorithms? What are examples of each case?
- The way in which these paradigms differ is in how they solve the problem instances: D&C divides problem instance into smaller sub-instances of the same problem, solve these recursively, and then put solutions together to a solution of the given instance; and Greedy algorithms finds the solution by always making the choice that looks optimal at the moment — don’t look ahead, never go back. On the other hand, DP turns recursion upside down. \nSome examples include: \nD&C: Mergesort, Quicksort, Strassen’s algorithm, FFT. \nGA: Prim’s algorithm, Kruskal’s algorithm. \nDP: Floyd-Warshall algorithm for the all pairs shortest path problem.

• What is the Matrix Chain Multiplication Problem?
- It is the problem that, given as input a sequence of matrices A_1, . . . , A_n, where A_i is a p_{i−1} × p_i-matrix; outputs the optimal number of multiplications needed to compute A_1 · A_2 · · · A_n, and an optimal parenthesisation to realise this.

• What are the "attempted" solutions to the MCM problem?
- Exhaustive search (CORRECT but SLOW): Try all possible parenthesisations and compare them. Running time is Ω(3^n) \nGreedy algorithm (INCORRECT): Always do the cheapest multiplication first. Does not work correctly — sometimes, it returns a parenthesisation that is not optimal. \nAlternative greedy algorithm (INCORRECT). Set outermost parentheses such that cheapest multiplication is done last. \nRecursive (D&C)(SLOW)

4 Network Flows

• Define a flow network: 
- A flow network consists of: \nA directed graph G = (V, E). \nA capacity function c : V × V → R such that c(u, v) ≥ 0 if (u, v) ∈ E and c(u, v) = 0 for all (u, v) ∈/ E. \nTwo distinguished vertices s,t ∈ V called the source and the sink, respectively.

• Define a flow in N:
- Let N = (G = (V, E), c,s,t) be a flow network. A flow in N is a function f : V × V → R satisfying the following conditions: \nCapacity constraint: f(u, v) ≤ c(u, v) for all u, v ∈ V. \nSkew symmetry: f(u, v) = −f(v, u) for all u, v ∈ V. \nFlow conservation: For all u ∈ V \ {s,t}, \sum{v∈V}{f(u, v) = 0}.

• Define the net flow and value of a flow:
- Let N = (G = (V, E), c,s,t) be flow network, f : V × V → R flow in N. \nFor u, v ∈ V we call f(u, v) the net flow at (u, v). \nThe value of the flow f is the number |f| = \nsum{v∈V}{f(s, v)}

• What is the Maximum-Flow problem?
- Given as input a network N, output the flow of maximum value in N. More specifically, find the flow f such that |f| = \sum{v∈V}{f(s, v)} is the largest possible (over all “legal” flows).

• Lay out Lemma 1 of Network Flows (set-of-vertices):
- N = (G = (V, E), c,s,t) flow network, f flow in N. Then for all X, Y, Z ⊆ V , \n1. f(X, X) = 0. \n2. f(X, Y) = −f(Y, X). \n3. If X ∩ Y = ∅ then \nf(X ∪ Y, Z) = f(X, Z) + f(Y, Z),\nf(Z, X ∪ Y) = f(Z, X) + f(Z, Y).

• Define residual networks:
-  N = (G = (V, E), c,s,t) flow network, f flow in N. \n1. For all u, v ∈ V × V, the residual capacity of (u, v) is c_f(u, v) = c(u, v) − f(u, v). \n2. The residual network of N induced by f is N_f = (G'=(V, E_f), c_f,s,t), where E_f = {(u, v) ∈ V × V | c_f(u, v) > 0}

• Lay out Lemma 2 of Network Flows (additive property):
- Let N = (G = (V, E), c,s,t) be a flow network. Let f be a flow in N. Let g : V × V → R be a flow in the residual network N_f. \nThen the function f + g : V × V → R defined by (f + g)(u, v) = f (u, v) + g(u, v) is a flow of value |f| + |g| in N.

• Define the concept of augmenting paths: 
- N = (G = (V, E), c,s,t) flow network, f flow in N. Then an augmenting path for f is a path P from s to t in the residual network N_f. \nThe residual capacity of P is c_f(P) = min{c_f(u, v) | (u, v) edge on P}.

• Lay out Lemma 3 of Network Flows (pushing flow through an augmenting path):
- N = (G = (V, E), c,s,t) flow network, f flow in N. P augmenting path. Then f_P : V × V → R defined by \nf_P(u, v) = cf(P) if (u, v) is an edge of P, \nf_P(u, v) = −cf (P) if (v, u) is an edge of P, \nf_P(u, v) = 0 otherwise \nis a flow in N_f of value c_f (P).

• Define the concept of a cut:
- N = (G = (V, E), c,s,t) flow network. A cut of N is a pair (S,T) such that: \n1. s ∈ S and t ∈ T, \n2. V = S ∪ T and S ∩ T = ∅. \nThe capacity of the cut (S,T) is c(S,T) = \sum{u∈S,v∈T}{c(u, v)}

• Lay out Lemma 4 of Network Flows (cut):
- N = (G = (V, E), c,s,t) flow network, f flow in N, (S,T) cut of N. Then |f| = f(S,T).

• Lay out the Max-Flow Min-Cut Theorem:
- Let N = (G = (V, E), c,s,t) be a flow network. Then the maximum value of a flow in N is equal to the minimum capacity of a cut in N.

• When will the Ford-Fulkerson algorithm terminate?
- Let f* be a maximum flow in a network N. \n1. If all capacities are integers, then Ford-Fulkerson stops after at most |f*| iterations of the main loop. \n2. If all capacities are rationals, then Ford-Fulkerson stops after at most q · |f*| iterations of the main loop, where q is the least common multiple of the denominators of all the capacities. \n3. For arbitrary real capacities, it may happen that Ford-Fulkerson does not stop.

• Explain the Edmonds-Karp Heuristic:
- The idea is to always choose a shortest augmenting path. If n number of vertices, m number of edges, then ≤ m + 1. A shortest augmenting path can be found by Breadth-First-Search in time O(n + m) = O(m).

• What is the running time of the Ford-Fulkerson algorithm with the Edmonds-Karp heuristic?
- Since it stops after at most O(nm) iterations of the main loop, the running time is O(nm^2).

5 MST: Prim's Algorithm

• What is the definition of a weighted graph?
- A weighted (directed or undirected graph) is a pair (G, W) consisting of a graph G = (V, E) and a weight function W : E → R. 

• What is the problem of connecting sites?
- Given a collection of sites and costs of connecting them, find a minimum cost way of connecting all sites.

• Explain the concept of spanning trees:
- G = (V, E) undirected connected graph and W weight function. H = (V^H, E^H) with V^H ⊆ V and E^H ⊆ E subgraph of G. \nThe weight of H is the number W(H) = \sum{e∈E^H}{W(e)}. \nH is a spanning subgraph of G if V^H = V

• Define the concept of Minimum Spanning Trees. What is the minimum spanning tree problem?:
- A minimum spanning tree (MST) of G is a connected spanning subgraph T of G of minimum weight. \nThe minimum spanning tree problem: \nGiven: Undirected connected weighted graph (G, W) Output: An MST of G

• What is a Priority Queue, and what methods are supported?
- A Priority Queue is an ADT for storing a collection of elements with an associated key. \nThe following methods are supported: \nInsert(e, k): Insert element e with key k. \nGet-Min(): Return an element with minimum key; an error occurs if the priority queue is empty. \nExtract-Min(): Return and remove an element with minimum key; an error if the priority queue is empty. \nIs-Empty(): Return true if the priority queue is empty and false otherwise. \nTo update the keys during the execution of Prim, we need priority queues supporting the following additional method: \nDecrease-Key(e, k): Set the key of e to k and update the priority queue. It is assumed that k is smaller than or equal to the old key of e.

6 MST: Kruskal's Algorithm

• What is a forest in the context of MSTs?
- A forest is a graph whose connected components are trees. The idea is the following: Starting from a spanning forest with no edges, repeatedly add edges of minimum weight (never creating a cycle) until the forest becomes a tree.

• Explain the data structures for disjoint sets along with the permitted operations:
-  A disjoint set data structure maintains a collection S = {S_1, . . . , S_k} of disjoint sets. The sets are dynamic, i.e., they may change over time. Each set S_i is identified by some representative, which is some member of that set. \nThe operations supported are the following: \nMake-Set(x): Creates new set whose only member is x. The representative is x. \nUnion(x, y): Unites set S_x containing x and set S_y containing y into a new set S and removes S_x and S_y from the collection. \nFind-Set(x): Returns representative of the set holding x.

• What is the idea of weighted-union heuristic?
- Maintaining a “length” field for each list. To execute Union(x, y), append shorter list to longer one (breaking ties arbitrarily).

• What Theorem is derived from the concept of weighted-union heuristic?
- Using the linked-list representation of disjoint sets and the weighted-union heuristic, a sequence of m Make-Set, Union & Find-Set operations, n of which are Make-Set operations, takes O(m + nlgn) time.

7 Computational Geometry

• Notions and basic definitions in Computational Geometry:
- 1. Points are pairs (x, y) with x, y ∈ R. \n2. A convex combination of two points p_1 = (x_1, y_1) and p_2 = (x_2, y_2) is a point p = (x, y) such that \nx = αx_1 + (1 − α)x_2 \ny = αy_1 + (1 − α)y_2 for some 0 ≤ α ≤ 1. \nAbbreviate to p = αp_1 + (1 − α)p_2. Intuitively, a point p is a convex combination of p_1 and p_2 if it is on the line segment from p_1 to p_2

• Define the concept of convex hull, and state the Convex Hull Problem:
- A set C of points is convex if for all p, q ∈ C the whole line segment pq is contained in C. The convex hull of a set Q of points is the smallest convex set C that contains Q. \n The Convex Hull Problem consists in, given as input a finite set Q of points in the plane, output the vertices of the convex hull of Q in counterclockwise order.

• What are Polar Angles?
- The polar angle of a point q with respect to a point p is the anti-clockwise angle between a horizontal line and the line through p and q.

• What is the idea of Graham's Scan?
-  Let p_0 be a “bottom-most” point in the set. Start walking around the points in the order of increasing polar angles. As long as you turn left, keep on walking. If you have to turn right to reach the next point, discard the current point and step back to the previous point. Repeat this until you can turn left to the next point. The points that remain are the vertices of the convex hull.
 



